import unittest
from datetime import datetime, timedelta
from pyspark.sql import Row
from optimal_config.create_spark_instance import generate_spark_instance
from your_module import check_file_processed, mark_file_as_processed, get_latest_file_info, check_for_new_files, main

class TestFileProcessing(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        # Set up a Spark instance for testing
        cls.spark = generate_spark_instance(100, 1)
        cls.schema = "test_schema"
        cls.schemafromyaml = "test_schemafromyaml"
        cls.hdfs_path = "hdfs:///path/to/your/directory"
        
        # Create a test DataFrame and write it to Hive
        data = [Row(filename='test_file.csv', processed_on=datetime.now())]
        df = cls.spark.createDataFrame(data)
        write_to_hive(df, cls.schema, "table_name", cls.schemafromyaml)
        
    @classmethod
    def tearDownClass(cls):
        # Stop the Spark session
        cls.spark.stop()
        
    def test_check_file_processed(self):
        # Test if the file is correctly identified as processed
        result = check_file_processed(self.spark, self.schema, 'test_file.csv')
        self.assertTrue(result)
        
    def test_mark_file_as_processed(self):
        # Mark a new file as processed
        mark_file_as_processed(self.spark, self.schema, 'new_test_file.csv', self.schemafromyaml)
        
        # Test if the new file is correctly marked as processed
        result = check_file_processed(self.spark, self.schema, 'new_test_file.csv')
        self.assertTrue(result)
        
    def test_get_latest_file_info(self):
        # Assuming files are created for testing in HDFS
        latest_file, latest_mod_time = get_latest_file_info(self.spark, self.hdfs_path)
        
        # Simple check if a file is retrieved
        self.assertIsNotNone(latest_file)
        self.assertIsNotNone(latest_mod_time)
        
    def test_check_for_new_files(self):
        # Insert a file record using write_to_hive function
        data = [Row(filename='72648_test_file.csv', processed_on=datetime.now())]
        df = self.spark.createDataFrame(data)
        write_to_hive(df, self.schema, "table_name", self.schemafromyaml)
        
        # Test if the function correctly identifies a new file
        result = check_for_new_files(self.spark, self.schema, self.hdfs_path, self.schemafromyaml)
        self.assertTrue(result)
        
    def test_main(self):
        try:
            main()
        except AssertionError as e:
            self.assertIn("No new files meeting the criteria", str(e))

if __name__ == '__main__':
    unittest.main()
