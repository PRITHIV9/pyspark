from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Initialize SparkSession
spark = SparkSession.builder.appName("OptimizeCount").getOrCreate()

# Sample DataFrame (replace with actual data)
data = [
    ("2023-01-31",), ("2023-02-28",), ("2023-03-31",), 
    # Add more sample data as needed
]
df = spark.createDataFrame(data, ["endofmonth"])

# Number of months expected
months = 30

# Cache the DataFrame to optimize multiple operations
df.cache()

# Repartition the DataFrame based on the 'endofmonth' column
df = df.repartition("endofmonth")

# Aggregate to get the distinct count of 'endofmonth'
distinct_endofmonth_count = df.select("endofmonth").agg(F.countDistinct("endofmonth")).collect()[0][0]

# Assert the count
assert distinct_endofmonth_count == months

# Optionally, unpersist the DataFrame if no longer needed
df.unpersist()

# Stop the SparkSession
spark.stop()
