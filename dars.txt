import os
import json
from pyspark.sql import SparkSession
from datetime import datetime, timedelta

# Define paths and file handling functions
def get_processed_files_tracker_path():
    return "/path/to/your/processed_files_tracker.json"

def load_processed_files():
    path = get_processed_files_tracker_path()
    if os.path.exists(path):
        with open(path, 'r') as file:
            return json.load(file)
    return {}

def save_processed_files(data):
    path = get_processed_files_tracker_path()
    with open(path, 'w') as file:
        json.dump(data, file, indent=4)

# Check if the file was already processed
def is_file_processed(file_name):
    processed_files = load_processed_files()
    return processed_files.get(file_name, False)

def mark_file_as_processed(file_name):
    processed_files = load_processed_files()
    processed_files[file_name] = True
    save_processed_files(processed_files)

# Fetch latest file information and process
def get_latest_file_info(spark, hdfs_path):
    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
    path = spark._jvm.org.apache.hadoop.fs.Path(hdfs_path)
    status = fs.listStatus(path)
    files_with_times = [(file.getPath().toString(), datetime.fromtimestamp(file.getModificationTime() / 1000))
                        for file in status if file.getPath().getName().startswith("72648") and file.getPath().getName().lower().endswith(".csv")]
    
    if not files_with_times:
        return None, None
    return sorted(files_with_times, key=lambda x: x[1], reverse=True)[0]

def check_for_new_files(spark, hdfs_path):
    latest_file, latest_mod_time = get_latest_file_info(spark, hdfs_path)
    if not latest_file:
        return False
    
    today = datetime.now().date()
    yesterday = today - timedelta(days=1)
    file_date_is_valid = latest_mod_time.date() in [today, yesterday]
    file_not_processed = not is_file_processed(latest_file)
    
    if file_date_is_valid and file_not_processed:
        mark_file_as_processed(latest_file)
        return True
    return False

def main():
    spark = SparkSession.builder.appName("Check New Files in HDFS with Criteria").getOrCreate()
    hdfs_path = "hdfs:///path/to/your/directory"
    
    new_files_found = check_for_new_files(spark, hdfs_path)
    spark.stop()
    
    assert new_files_found, "No new files meeting the criteria were found or file already processed."

if __name__ == "__main__":
    main()
