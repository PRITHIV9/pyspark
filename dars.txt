import unittest
from unittest.mock import MagicMock, patch
from datetime import datetime, timedelta
from pyspark.sql import SparkSession, Row
from optimal_config.create_spark_instance import generate_spark_instance

class TestFileProcessing(unittest.TestCase):

    @patch('optimal_config.create_spark_instance.generate_spark_instance')
    @patch('builtins.datetime')
    def setUp(self, mock_datetime, mock_generate_spark_instance):
        # Mocking the spark session creation
        self.spark = MagicMock(SparkSession)
        mock_generate_spark_instance.return_value = self.spark

        # Mocking current date
        self.mocked_now = datetime(2024, 5, 24)
        mock_datetime.now.return_value = self.mocked_now

        # Setting up schema and schemafromyaml for testing
        self.schema = "test_schema"
        self.schemafromyaml = "test_schemafromyaml"

        # HDFS path
        self.hdfs_path = "hdfs:///path/to/your/directory"

    @patch('optimal_config.create_spark_instance.generate_spark_instance')
    def test_check_file_processed(self, mock_generate_spark_instance):
        from your_script import check_file_processed

        mock_df = MagicMock()
        self.spark.sql.return_value = mock_df

        file_name = "test_file.csv"
        mock_df.filter.return_value.count.return_value = 1

        result = check_file_processed(self.spark, self.schema, file_name)
        self.assertTrue(result)

        mock_df.filter.return_value.count.return_value = 0
        result = check_file_processed(self.spark, self.schema, file_name)
        self.assertFalse(result)

    @patch('your_script.write_to_hive')
    def test_mark_file_as_processed(self, mock_write_to_hive):
        from your_script import mark_file_as_processed

        file_name = "test_file.csv"

        mark_file_as_processed(self.spark, self.schema, file_name, self.schemafromyaml)
        mock_write_to_hive.assert_called_once()

    @patch('your_script.get_latest_file_info')
    @patch('your_script.check_file_processed')
    @patch('your_script.mark_file_as_processed')
    def test_check_for_new_files(self, mock_mark_file_as_processed, mock_check_file_processed, mock_get_latest_file_info):
        from your_script import check_for_new_files

        mock_get_latest_file_info.return_value = ("test_file.csv", self.mocked_now)
        mock_check_file_processed.return_value = False

        result = check_for_new_files(self.spark, self.schema, self.hdfs_path, self.schemafromyaml)
        self.assertTrue(result)
        mock_mark_file_as_processed.assert_called_once()

        mock_check_file_processed.return_value = True
        result = check_for_new_files(self.spark, self.schema, self.hdfs_path, self.schemafromyaml)
        self.assertFalse(result)

    @patch('your_script.check_for_new_files')
    @patch('optimal_config.create_spark_instance.generate_spark_instance')
    def test_main(self, mock_generate_spark_instance, mock_check_for_new_files):
        from your_script import main

        mock_check_for_new_files.return_value = True

        main()
        mock_check_for_new_files.assert_called_once()

if __name__ == "__main__":
    unittest.main()
