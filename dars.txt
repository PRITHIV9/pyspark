from pyspark.sql import SparkSession

# Initialize SparkSession with desired configurations
spark = SparkSession.builder \
    .appName("OptimizeCount") \
    .config("spark.executor.memory", "600G") \
    .config("spark.executor.cores", "40") \
    .config("spark.executor.instances", "1") \
    .getOrCreate()

# Sample DataFrame (replace with actual data)
data = [
    ("2023-01-31",), ("2023-02-28",), ("2023-03-31",), 
    # Add more sample data as needed
]
df = spark.createDataFrame(data, ["endofmonth"])

# Number of months expected
months = 30

# Cache the DataFrame to optimize multiple operations
df.cache()

# Determine the size of the DataFrame
data_size = df.count()  # Number of rows
df_bytes = df.rdd.map(lambda row: len(str(row))).reduce(lambda x, y: x + y)
df_size_mb = df_bytes / (1024 * 1024)
print(f"Number of rows in DataFrame: {data_size}")
print(f"Approximate DataFrame size: {df_size_mb} MB")

# Check the cluster configuration
sc = spark.sparkContext

# Verify configurations
executor_memory = sc._conf.get("spark.executor.memory")
executor_cores = sc._conf.get("spark.executor.cores")
num_executors = sc._conf.get("spark.executor.instances")

print(f"Executor memory: {executor_memory}")
print(f"Executor cores: {executor_cores}")
print(f"Number of executors: {num_executors}")

# Specify the number of partitions
num_partitions = 10  # Adjust this based on your data size and cluster resources

# Repartition the DataFrame based on the 'endofmonth' column and specified number of partitions
df = df.repartition(num_partitions, "endofmonth")

# Stop the SparkSession
spark.stop()
