import unittest
from unittest.mock import MagicMock, patch
from datetime import datetime, timedelta
from your_script import check_file_processed, mark_file_as_processed, check_for_new_files, get_latest_file_info

class TestFileProcessing(unittest.TestCase):
    def setUp(self):
        # Set up mock Spark session and other components
        self.spark = MagicMock()
        self.hdfs_path = "hdfs:///path/to/your/directory"
        self.file_name = "72648_testfile.csv"
        self.today = datetime.now().date()
        self.yesterday = self.today - timedelta(days=1)

    @patch('your_script.spark.sql')
    def test_check_file_processed(self, mock_sql):
        # Set up the DataFrame mock to simulate Spark SQL behavior
        df_mock = MagicMock()
        df_mock.count.return_value = 1
        mock_sql.return_value = df_mock

        # Invocation
        result = check_file_processed(self.spark, self.file_name)

        # Check
        self.assertTrue(result)
        mock_sql.assert_called_with(f"SELECT * FROM processed_files_tracking WHERE file_name = '{self.file_name}'")

    @patch('your_script.spark.sql')
    def test_mark_file_as_processed(self, mock_sql):
        # Set up the function to not raise any exceptions
        mock_sql.return_value = None

        # Invocation
        mark_file_as_processed(self.spark, self.file_name)

        # Check
        mock_sql.assert_called()

    @patch('your_script.get_latest_file_info')
    @patch('your_script.check_file_processed')
    @patch('your_script.mark_file_as_processed')
    def test_check_for_new_files(self, mock_mark, mock_check, mock_latest_info):
        # Setup
        mock_latest_info.return_value = (self.file_name, datetime.now())
        mock_check.return_value = False

        # Invocation
        result = check_for_new_files(self.spark, self.hdfs_path)

        # Check
        self.assertTrue(result)
        mock_mark.assert_called_once_with(self.spark, self.file_name)
        mock_latest_info.assert_called_once_with(self.spark, self.hdfs_path)

    @patch('your_script.spark._jvm.org.apache.hadoop.fs.FileSystem.get')
    def test_get_latest_file_info(self, mock_fs_get):
        # Setup the FileSystem mock to simulate HDFS file listing
        mock_status = MagicMock()
        mock_path = MagicMock()
        mock_path.getName.return_value = "72648_testfile.csv"
        mock_path.toString.return_value = "hdfs:///path/to/your/directory/72648_testfile.csv"
        mock_status.getPath.return_value = mock_path
        mock_status.getModificationTime.return_value = int((datetime.now() - timedelta(hours=1)).timestamp() * 1000)
        mock_fs_get.return_value.listStatus.return_value = [mock_status]

        # Invocation
        file_name, mod_time = get_latest_file_info(self.spark, self.hdfs_path)

        # Check
        self.assertEqual(file_name, "72648_testfile.csv")
        self.assertIsNotNone(mod_time)

if __name__ == '__main__':
    unittest.main()
