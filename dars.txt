from datetime import datetime, timedelta
from pyspark.sql import SparkSession, functions as F

def check_file_processed(spark, file_name):
    """Check if the given file name has been processed by querying the Hive table."""
    query = f"SELECT * FROM processed_files_tracking WHERE file_name = '{file_name}'"
    df = spark.sql(query)
    return df.count() > 0

def mark_file_as_processed(spark, file_name):
    """Mark a file as processed by adding it to the Hive tracking table."""
    processed_on = datetime.now()
    spark.sql(f"INSERT INTO processed_files_tracking VALUES ('{file_name}', '{processed_on}')")

def get_latest_file_info(spark, hdfs_path):
    """Retrieve the latest modified file that matches specific criteria from HDFS."""
    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
    path = spark._jvm.org.apache.hadoop.fs.Path(hdfs_path)
    status = fs.listStatus(path)
    files_with_times = [
        (file.getPath().getName(), datetime.fromtimestamp(file.getModificationTime() / 1000))
        for file in status if file.getPath().getName().startswith("72648") and file.getPath().getName().lower().endswith(".csv")
    ]
    if not files_with_times:
        return None, None
    return sorted(files_with_times, key=lambda x: x[1], reverse=True)[0]

def check_for_new_files(spark, hdfs_path):
    """Check if there are any new files that have not been processed."""
    latest_file, latest_mod_time = get_latest_file_info(spark, hdfs_path)
    if not latest_file:
        return False
    
    today = datetime.now().date()
    yesterday = today - timedelta(days=1)
    file_date_is_valid = latest_mod_time.date() in [today, yesterday]
    file_not_processed = not check_file_processed(spark, latest_file)
    
    if file_date_is_valid and file_not_processed:
        mark_file_as_processed(spark, latest_file)
        return True
    return False

def main():
    spark = SparkSession.builder.appName("Check New Files in HDFS with Criteria").enableHiveSupport().getOrCreate()
    hdfs_path = "hdfs:///path/to/your/directory"
    new_files_found = check_for_new_files(spark, hdfs_path)
    spark.stop()
    
    assert new_files_found, "No new files meeting the criteria were found or file already processed."

if __name__ == "__main__":
    main()
import unittest
from unittest.mock import MagicMock, patch
from datetime import datetime
from your_script import check_file_processed, mark_file_as_processed, check_for_new_files, get_latest_file_info

class TestFileProcessing(unittest.TestCase):
    def setUp(self):
        # Setup a mock Spark session
        self.spark = MagicMock()
        self.hdfs_path = "hdfs:///path/to/your/directory"
        self.file_name = "72648_testfile.csv"

    @patch('your_script.SparkSession')
    def test_check_file_processed(self, mock_spark):
        # Setup
        df_mock = MagicMock()
        df_mock.count.return_value = 1
        mock_spark.sql.return_value = df_mock

        # Invocation
        result = check_file_processed(self.spark, self.file_name)

        # Check
        self.assertTrue(result)
        df_mock.sql.assert_called_with(f"SELECT * FROM processed_files_tracking WHERE file_name = '{self.file_name}'")

    @patch('your_script.SparkSession')
    def test_mark_file_as_processed(self, mock_spark):
        # Setup
        mock_spark.sql = MagicMock()

        # Invocation
        mark_file_as_processed(self.spark, self.file_name)

        # Check
        mock_spark.sql.assert_called_once()

    @patch('your_script.get_latest_file_info')
    @patch('your_script.check_file_processed')
    @patch('your_script.mark_file_as_processed')
    def test_check_for_new_files(self, mock_mark, mock_check, mock_latest_info):
        # Setup
        mock_latest_info.return_value = (self.file_name, datetime.now())
        mock_check.return_value = False

        # Invocation
        result = check_for_new
