from datetime import datetime, timedelta
from pyspark.sql import SparkSession, Row
from pyspark.sql import types as T
from pyspark.sql import functions as F
from optimal_config.create_spark_instance import generate_spark_instance

def check_file_processed(spark, schema, file_name):
    """Check if the given file name has been processed by querying the Hive table."""
    print(f"Checking if the file '{file_name}' has been processed...")
    query = f"SELECT * FROM {schema}.table_name"
    df = spark.sql(query)
    file_processed = df.filter(F.col("filename") == file_name).count() > 0
    if file_processed:
        print(f"File '{file_name}' has already been processed.")
    else:
        print(f"File '{file_name}' has not been processed yet.")
    return file_processed

def mark_file_as_processed(spark, schema, file_name, schemafromyaml):
    """Mark a file as processed by adding it to the Hive tracking table."""
    print(f"Marking file '{file_name}' as processed...")
    processed_on = datetime.now()
    schema = T.StructType([
        T.StructField("filename", T.StringType(), True),
        T.StructField("processed_on", T.TimestampType(), True)
    ])
    df = spark.createDataFrame([(file_name, processed_on)], schema)
    write_to_hive(df, schema, "table_name", schemafromyaml)
    print(f"File '{file_name}' marked as processed at {processed_on}.")

def get_latest_file_info(spark, hdfs_path):
    """Retrieve the latest modified file that matches specific criteria from HDFS."""
    print(f"Retrieving the latest modified file from HDFS path: {hdfs_path}...")
    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
    path = spark._jvm.org.apache.hadoop.fs.Path(hdfs_path)
    status = fs.listStatus(path)
    files_with_times = [
        (file.getPath().getName(), datetime.fromtimestamp(file.getModificationTime() / 1000))
        for file in status if file.getPath().getName().startswith("72648") and file.getPath().getName().lower().endswith(".csv")
    ]
    if not files_with_times:
        print("No matching files found in the HDFS path.")
        return None, None
    latest_file, latest_mod_time = sorted(files_with_times, key=lambda x: x[1], reverse=True)[0]
    print(f"Latest file found: {latest_file} (modified on {latest_mod_time})")
    return latest_file, latest_mod_time

def check_for_new_files(spark, schema, hdfs_path, schemafromyaml):
    """Check if there are any new files that have not been processed."""
    latest_file, latest_mod_time = get_latest_file_info(spark, hdfs_path)
    if not latest_file:
        print("No new files found.")
        return False
    
    today = datetime.now().date()
    yesterday = today - timedelta(days=1)
    file_date_is_valid = latest_mod_time.date() in [today, yesterday]
    file_not_processed = not check_file_processed(spark, schema, latest_file)
    
    if file_date_is_valid and file_not_processed:
        print(f"New file '{latest_file}' found and it is within the valid date range.")
        mark_file_as_processed(spark, schema, latest_file, schemafromyaml)
        return True
    else:
        if not file_date_is_valid:
            print(f"File '{latest_file}' is not within the valid date range.")
        if not file_not_processed:
            print(f"File '{latest_file}' has already been processed.")
    return False

def main():
    hdfs_path = "hdfs:///path/to/your/directory"
    schema, schemafromyaml = create_variables()
    spark = generate_spark_instance(100, 1)
    print("Spark session created successfully.")
    
    new_files_found = check_for_new_files(spark, schema, hdfs_path, schemafromyaml)
    spark.stop()
    print("Spark session stopped.")
    
    if new_files_found:
        print("New files meeting the criteria were found and processed.")
    else:
        print("No new files meeting the criteria were found or they were already processed.")
    
    assert new_files_found, "No new files meeting the criteria were found or file already processed."

if __name__ == "__main__":
    main()

from datetime import datetime, timedelta
from pyspark.sql import SparkSession, Row
from pyspark.sql import types as T
from pyspark.sql import functions as F
from optimal_config.create_spark_instance import generate_spark_instance

def check_file_processed(spark, schema, file_name, location):
    """Check if the given file name has been processed by querying the Hive table."""
    print(f"Checking if the file '{file_name}' from location '{location}' has been processed...")
    query = f"SELECT * FROM {schema}.table_name"
    df = spark.sql(query)
    file_processed = df.filter((F.col("filename") == file_name) & (F.col("location") == location)).count() > 0
    if file_processed:
        print(f"File '{file_name}' from location '{location}' has already been processed.")
    else:
        print(f"File '{file_name}' from location '{location}' has not been processed yet.")
    return file_processed

def mark_file_as_processed(spark, schema, file_name, schemafromyaml, location):
    """Mark a file as processed by adding it to the Hive tracking table."""
    print(f"Marking file '{file_name}' from location '{location}' as processed...")
    processed_on = datetime.now()
    schema = T.StructType([
        T.StructField("filename", T.StringType(), True),
        T.StructField("processed_on", T.TimestampType(), True),
        T.StructField("location", T.StringType(), True)
    ])
    df = spark.createDataFrame([(file_name, processed_on, location)], schema)
    write_to_hive(df, schema, "table_name", schemafromyaml)
    print(f"File '{file_name}' from location '{location}' marked as processed at {processed_on}.")

def get_latest_file_info(spark, hdfs_path):
    """Retrieve the latest modified file that matches specific criteria from HDFS."""
    print(f"Retrieving the latest modified file from HDFS path: {hdfs_path}...")
    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
    path = spark._jvm.org.apache.hadoop.fs.Path(hdfs_path)
    status = fs.listStatus(path)
    files_with_times = [
        (file.getPath().getName(), datetime.fromtimestamp(file.getModificationTime() / 1000))
        for file in status if file.getPath().getName().startswith("72648") and file.getPath().getName().lower().endswith(".csv")
    ]
    if not files_with_times:
        print("No matching files found in the HDFS path.")
        return None, None
    latest_file, latest_mod_time = sorted(files_with_times, key=lambda x: x[1], reverse=True)[0]
    print(f"Latest file found: {latest_file} (modified on {latest_mod_time})")
    return latest_file, latest_mod_time

def check_for_new_files(spark, schema, hdfs_path, schemafromyaml, location):
    """Check if there are any new files that have not been processed."""
    latest_file, latest_mod_time = get_latest_file_info(spark, hdfs_path)
    if not latest_file:
        print("No new files found.")
        return False
    
    today = datetime.now().date()
    yesterday = today - timedelta(days=1)
    file_date_is_valid = latest_mod_time.date() in [today, yesterday]
    file_not_processed = not check_file_processed(spark, schema, latest_file, location)
    
    if file_date_is_valid and file_not_processed:
        print(f"New file '{latest_file}' found and it is within the valid date range.")
        mark_file_as_processed(spark, schema, latest_file, schemafromyaml, location)
        return True
    else:
        if not file_date_is_valid:
            print(f"File '{latest_file}' is not within the valid date range.")
        if not file_not_processed:
            print(f"File '{latest_file}' has already been processed.")
    return False

def main():
    hdfs_path = "hdfs:///path/to/your/directory"
    schema, schemafromyaml = create_variables()
    location = "hardcoded_location"  # Replace with the actual location from your YAML if available
    spark = generate_spark_instance(100, 1)
    print("Spark session created successfully.")
    
    new_files_found = check_for_new_files(spark, schema, hdfs_path, schemafromyaml, location)
    spark.stop()
    print("Spark session stopped.")
    
    if new_files_found:
        print("New files meeting the criteria were found and processed.")
    else:
        print("No new files meeting the criteria were found or they were already processed.")
    
    assert new_files_found, "No new files meeting the criteria were found or file already processed."

if __name__ == "__main__":
    main()
