import os
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
import pyodbc

# Database connection setup
def get_db_connection():
    """Return a connection to the SQL Server database."""
    conn = pyodbc.connect(
        'DRIVER={ODBC Driver 17 for SQL Server};'
        'SERVER=your_server;'
        'DATABASE=your_database;'
        'UID=your_username;'
        'PWD=your_password'
    )
    return conn

# Load processed files from the database
def load_processed_files():
    """Load processed files from the database and return them as a set."""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT file_name FROM ProcessedFiles")
    processed_files = {row.file_name for row in cursor.fetchall()}
    conn.close()
    return processed_files

# Save a new processed file to the database
def save_processed_file(file_name):
    """Save the processed file to the database."""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("INSERT INTO ProcessedFiles (file_name) VALUES (?)", (file_name,))
    conn.commit()
    conn.close()

# Check if a file has already been processed
def is_file_processed(file_name):
    """Check if the given file name is in the set of processed files."""
    processed_files = load_processed_files()
    return file_name in processed_files

# Mark a file as processed in the database
def mark_file_as_processed(file_name):
    """Mark a file as processed by adding it to the database."""
    save_processed_file(file_name)

# Get the latest file information from HDFS based on naming and modification time
def get_latest_file_info(spark, hdfs_path):
    """Retrieve the latest modified file that matches specific criteria from HDFS."""
    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
    path = spark._jvm.org.apache.hadoop.fs.Path(hdfs_path)
    status = fs.listStatus(path)
    files_with_times = [
        (file.getPath().toString(), datetime.fromtimestamp(file.getModificationTime() / 1000))
        for file in status if file.getPath().getName().startswith("72648") and file.getPath().getName().lower().endswith(".csv")
    ]
    if not files_with_times:
        return None, None  # Return None if no files match
    return sorted(files_with_times, key=lambda x: x[1], reverse=True)[0]

# Extract the file name from the full HDFS path
def extract_file_name(full_path):
    """Extract the file name from the full HDFS path."""
    return full_path.split('/')[-1]

# Check for new files that haven't been processed
def check_for_new_files(spark, hdfs_path):
    """Check if there are any new files that have not been processed."""
    latest_file, latest_mod_time = get_latest_file_info(spark, hdfs_path)
    if not latest_file:
        print("No matching files found in HDFS.")
        return False

    # Extract the file name
    file_name = extract_file_name(latest_file)

    today = datetime.now().date()
    yesterday = today - timedelta(days=1)
    file_date_is_valid = latest_mod_time.date() in [today, yesterday]
    file_not_processed = not is_file_processed(file_name)

    if file_date_is_valid and file_not_processed:
        mark_file_as_processed(file_name)
        print(f"New file found and marked as processed: {file_name}")
        return True
    print("No new files to process or file already processed.")
    return False

# Main function to execute the job
def main():
    """Main function to orchestrate checking and processing of new files."""
    spark = SparkSession.builder.appName("Check New Files in HDFS with Criteria").getOrCreate()
    hdfs_path = "hdfs:///path/to/your/directory"
    new_files_found = check_for_new_files(spark, hdfs_path)
    spark.stop()
    
    # Ensure the job fails if no new files meeting the criteria were found
    assert new_files_found, "No new files meeting the criteria were found or file already processed."

if __name__ == "__main__":
    main()
import os
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
import pyodbc

class FileProcessor:
    def __init__(self, db_config, hdfs_path):
        self.db_config = db_config
        self.hdfs_path = hdfs_path
        self.spark = SparkSession.builder.appName("Check New Files in HDFS with Criteria").getOrCreate()
    
    def get_db_connection(self):
        """Return a connection to the SQL Server database."""
        conn = pyodbc.connect(
            f"DRIVER={{ODBC Driver 17 for SQL Server}};"
            f"SERVER={self.db_config['server']};"
            f"DATABASE={self.db_config['database']};"
            f"UID={self.db_config['username']};"
            f"PWD={self.db_config['password']}"
        )
        return conn
    
    def load_processed_files(self):
        """Load processed files from the database and return them as a set."""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT file_name FROM ProcessedFiles")
        processed_files = {row.file_name for row in cursor.fetchall()}
        conn.close()
        return processed_files

    def save_processed_file(self, file_name):
        """Save the processed file to the database."""
        conn = self.get_db_connection()
        cursor = conn.cursor()
        cursor.execute("INSERT INTO ProcessedFiles (file_name) VALUES (?)", (file_name,))
        conn.commit()
        conn.close()

    def is_file_processed(self, file_name):
        """Check if the given file name is in the set of processed files."""
        processed_files = self.load_processed_files()
        return file_name in processed_files

    def mark_file_as_processed(self, file_name):
        """Mark a file as processed by adding it to the database."""
        self.save_processed_file(file_name)

    def get_latest_file_info(self):
        """Retrieve the latest modified file that matches specific criteria from HDFS."""
        fs = self.spark._jvm.org.apache.hadoop.fs.FileSystem.get(self.spark._jsc.hadoopConfiguration())
        path = self.spark._jvm.org.apache.hadoop.fs.Path(self.hdfs_path)
        status = fs.listStatus(path)
        files_with_times = [
            (file.getPath().toString(), datetime.fromtimestamp(file.getModificationTime() / 1000))
            for file in status if file.getPath().getName().startswith("72648") and file.getPath().getName().lower().endswith(".csv")
        ]
        if not files_with_times:
            return None, None  # Return None if no files match
        return sorted(files_with_times, key=lambda x: x[1], reverse=True)[0]

    def extract_file_name(self, full_path):
        """Extract the file name from the full HDFS path."""
        return full_path.split('/')[-1]

    def check_for_new_files(self):
        """Check if there are any new files that have not been processed."""
        latest_file, latest_mod_time = self.get_latest_file_info()
        if not latest_file:
            print("No matching files found in HDFS.")
            return False

        # Extract the file name
        file_name = self.extract_file_name(latest_file)

        today = datetime.now().date()
        yesterday = today - timedelta(days=1)
        file_date_is_valid = latest_mod_time.date() in [today, yesterday]
        file_not_processed = not self.is_file_processed(file_name)

        if file_date_is_valid and file_not_processed:
            self.mark_file_as_processed(file_name)
            print(f"New file found and marked as processed: {file_name}")
            return True
        print("No new files to process or file already processed.")
        return False

    def process_files(self):
        """Main function to orchestrate checking and processing of new files."""
        new_files_found = self.check_for_new_files()
        self.spark.stop()
        
        # Ensure the job fails if no new files meeting the criteria were found
        assert new_files_found, "No new files meeting the criteria were found or file already processed."

if __name__ == "__main__":
    db_config = {
        'server': 'your_server',
        'database': 'your_database',
        'username': 'your_username',
        'password': 'your_password'
    }
    hdfs_path = "hdfs:///path/to/your/directory"
    processor = FileProcessor(db_config, hdfs_path)
    processor.process_files()
