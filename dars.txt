table: processed_files_tracking
schema: default
columns:
  - name: file_name
    type: string
  - name: processed_on
    type: timestamp
storage:
  format: parquet
  location: hdfs:///path/to/hive/tables/processed_files_tracking

from datetime import datetime, timedelta
from pyspark.sql import SparkSession, functions as F
import yaml

def load_yaml_config(path):
    """Load the YAML configuration for Hive table schema and settings."""
    with open(path, 'r') as file:
        return yaml.safe_load(file)

def get_tracking_table_name():
    """Returns the Hive table name used for tracking processed files."""
    return "processed_files_tracking"

def check_file_processed(spark, file_name):
    """Check if the given file name has been processed by querying the Hive table."""
    df = read_from_hdfs(get_tracking_table_name(), schema="default", env="prod")
    return df.filter(F.col("file_name") == file_name).count() > 0

def mark_file_as_processed(spark, file_name, schema_from_yaml):
    """Mark a file as processed by adding it to the Hive tracking table."""
    config = load_yaml_config(schema_from_yaml)
    processed_df = spark.createDataFrame([(file_name, datetime.now())], schema="file_name STRING, processed_on TIMESTAMP")
    writeToHive(processed_df, config['table'], config['schema'], "prod", schema_from_yaml)

def get_latest_file_info(spark, hdfs_path):
    """Retrieve the latest modified file that matches specific criteria from HDFS."""
    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
    path = spark._jvm.org.apache.hadoop.fs.Path(hdfs_path)
    status = fs.listStatus(path)
    files_with_times = [
        (file.getPath().getName(), datetime.fromtimestamp(file.getModificationTime() / 1000))
        for file in status if file.getPath().getName().startswith("72648") and file.getPath().getName().lower().endswith(".csv")
    ]
    if not files_with_times:
        return None, None
    return sorted(files_with_times, key=lambda x: x[1], reverse=True)[0]

def check_for_new_files(spark, hdfs_path, schema_from_yaml):
    """Check if there are any new files that have not been processed."""
    latest_file, latest_mod_time = get_latest_file_info(spark, hdfs_path)
    if not latest_file:
        return False

    today = datetime.now().date()
    yesterday = today - timedelta(days=1)
    file_date_is_valid = latest_mod_time.date() in [today, yesterday]
    file_not_processed = not check_file_processed(spark, latest_file)

    if file_date_is_valid and file_not_processed:
        mark_file_as_processed(spark, latest_file, schema_from_yaml)
        return True
    return False

def main():
    spark = SparkSession.builder.appName("Check New Files in HDFS with Criteria").enableHiveSupport().getOrCreate()
    hdfs_path = "hdfs:///path/to/your/directory"
    schema_from_yaml = "/path/to/schema.yaml"
    new_files_found = check_for_new_files(spark, hdfs_path, schema_from_yaml)
    spark.stop()

    assert new_files_found, "No new files meeting the criteria were found or file already processed."

if __name__ == "__main__":
    main()
import unittest
from unittest.mock import MagicMock, patch
from datetime import datetime
from your_script import check_file_processed, mark_file_as_processed, check_for_new_files, get_latest_file_info

class TestFileProcessing(unittest.TestCase):
    def setUp(self):
        # Setup a mock Spark session
        self.spark = MagicMock()
        self.hdfs_path = "hdfs:///path/to/your/directory"
        self.file_name = "72648_testfile.csv"
        self.schema_from_yaml = "/path/to/schema.yaml"

    @patch('your_script.read_from_hdfs')
    def test_check_file_processed(self, mock_read):
        # Setup
        df_mock = MagicMock()
        df_mock.filter.return_value.count.return_value = 1
        mock_read.return_value = df_mock

        # Invocation
        result = check_file_processed(self.spark, self.file_name)

        # Check
        self.assertTrue(result)
        df_mock.filter.assert_called_with("file_name == '72648_testfile.csv'")
        mock_read.assert_called_once()

    @patch('your_script.writeToHive')
    @patch('your_script.load_yaml_config')
    def test_mark_file_as_processed(self, mock_load_yaml, mock_write):
        # Setup
        mock_load_yaml.return_value = {'table': 'processed_files_tracking', 'schema': 'default'}
        processed_df = MagicMock()

        # Invocation
        mark_file_as_processed(self.spark, self.file_name, self.schema_from_yaml)

        # Check
        mock_write.assert_called_once()
        args, kwargs = mock_write.call_args
        self.assertEqual(kwargs['table_name'], 'processed_files_tracking')
        self.assertEqual(kwargs['schema'], 'default')

    @patch('your_script.get_latest_file_info')
    @patch('your_script.check_file_processed')
    @patch('your_script.mark_file_as_processed')
    def test_check_for_new_files(self, mock_mark, mock_check, mock_latest_info):
        # Setup
        mock_latest_info.return_value = (self.file_name, datetime.now())
        mock_check.return_value = False

        # Invocation
        result = check_for_new_files(self.spark, self.hdfs_path, self.schema_from_yaml)

        # Check
        self.assertTrue(result)
        mock_mark.assert_called_once_with(self.spark, self.file_name, self.schema_from_yaml)
        mock_latest_info.assert_called_once_with(self.spark, self.hdfs_path)

    @patch('your_script.SparkSession')
    def test_get_latest_file_info(self, mock_spark):
        # Mocking deeper Spark functionality requires specific attention
        mock_spark.builder.getOrCreate.return_value._jvm.org.apache.hadoop.fs.FileSystem.get.return_value.listStatus.return_value = []

        # Invocation
        result = get_latest_file_info(self.spark, self.hdfs_path)

        # Check
        self.assertEqual(result, (None, None))

if __name__ == '__main__':
    unittest.main()
