umEmevREZ6U9nAG


from pyspark.sql import SparkSession
from datetime import datetime, timedelta
import os
import json

def get_latest_file_info(spark, hdfs_path):
    """ Fetches the latest file based on naming and modification time. """
    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
    path = spark._jvm.org.apache.hadoop.fs.Path(hdfs_path)
    status = fs.listStatus(path)
    files_with_times = [(file.getPath().toString(), datetime.fromtimestamp(file.getModificationTime() / 1000))
                        for file in status if file.getPath().getName().startswith("72648") and file.getPath().getName().lower().endswith(".csv")]
    if not files_with_times:
        return None, None
    latest_file, latest_time = sorted(files_with_times, key=lambda x: x[1], reverse=True)[0]
    return latest_file, latest_time

def check_for_new_files(spark, hdfs_path):
    """ Checks if the latest file was modified today or yesterday and matches naming criteria. """
    latest_file, latest_mod_time = get_latest_file_info(spark, hdfs_path)
    if latest_file is None:
        print("No matching files available.")
        return False
    today = datetime.now().date()
    yesterday = today - timedelta(days=1)
    if latest_mod_time.date() in [today, yesterday]:
        print(f"New file detected: {latest_file}, modified at {latest_mod_time}")
        return True
    else:
        print("No new files found today or yesterday.")
        return False

def get_processed_files():
    """ Retrieves a list of processed files from a JSON storage. """
    try:
        with open('processed_files.json', 'r') as file:
            processed_files = json.load(file)
    except FileNotFoundError:
        processed_files = {}
    return processed_files

def set_processed_file(file_name):
    """ Marks a file as processed by adding it to a JSON storage. """
    processed_files = get_processed_files()
    processed_files[file_name] = True
    with open('processed_files.json', 'w') as file:
        json.dump(processed_files, file)

def file_already_processed(file_name):
    """ Checks if a file has already been processed. """
    processed_files = get_processed_files()
    return processed_files.get(file_name, False)

def process_file(file_name):
    """ Processes the file if it hasn't been processed yet. """
    if file_already_processed(file_name):
        print(f"File {file_name} has already been processed.")
        return
    print(f"Processing file: {file_name}")
    # Insert actual file processing logic here
    set_processed_file(file_name)

def main():
    spark = SparkSession.builder.appName("Check and Process New Files").getOrCreate()
    hdfs_path = "hdfs:///path/to/your/directory"
    if check_for_new_files(spark, hdfs_path):
        process_file(latest_file)
    spark.stop()

if __name__ == "__main__":
    main()
